[Scrypted Documentation](../globals.md) / ChatCompletionCreateParamsNonStreaming

# Interface: ChatCompletionCreateParamsNonStreaming

## Extends

- `ChatCompletionCreateParamsBase`

## Properties

### messages

> **messages**: `ChatCompletionMessageParam`[]

A list of messages comprising the conversation so far. Depending on the
[model](https://platform.openai.com/docs/models) you use, different message
types (modalities) are supported, like
[text](https://platform.openai.com/docs/guides/text-generation),
[images](https://platform.openai.com/docs/guides/vision), and
[audio](https://platform.openai.com/docs/guides/audio).

#### Inherited from

`ChatCompletionCreateParamsBase.messages`

***

### model

> **model**: `string` & `object` \| `ChatModel`

Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
wide range of models with different capabilities, performance characteristics,
and price points. Refer to the
[model guide](https://platform.openai.com/docs/models) to browse and compare
available models.

#### Inherited from

`ChatCompletionCreateParamsBase.model`

***

### audio?

> `optional` **audio**: `null` \| `ChatCompletionAudioParam`

Parameters for audio output. Required when audio output is requested with
`modalities: ["audio"]`.
[Learn more](https://platform.openai.com/docs/guides/audio).

#### Inherited from

`ChatCompletionCreateParamsBase.audio`

***

### frequency\_penalty?

> `optional` **frequency\_penalty**: `null` \| `number`

Number between -2.0 and 2.0. Positive values penalize new tokens based on their
existing frequency in the text so far, decreasing the model's likelihood to
repeat the same line verbatim.

#### Inherited from

`ChatCompletionCreateParamsBase.frequency_penalty`

***

### ~~function\_call?~~

> `optional` **function\_call**: `"auto"` \| `"none"` \| `ChatCompletionFunctionCallOption`

#### Deprecated

Deprecated in favor of `tool_choice`.

Controls which (if any) function is called by the model.

`none` means the model will not call a function and instead generates a message.

`auto` means the model can pick between generating a message or calling a
function.

Specifying a particular function via `{"name": "my_function"}` forces the model
to call that function.

`none` is the default when no functions are present. `auto` is the default if
functions are present.

#### Inherited from

`ChatCompletionCreateParamsBase.function_call`

***

### ~~functions?~~

> `optional` **functions**: `Function`[]

#### Deprecated

Deprecated in favor of `tools`.

A list of functions the model may generate JSON inputs for.

#### Inherited from

`ChatCompletionCreateParamsBase.functions`

***

### logit\_bias?

> `optional` **logit\_bias**: `null` \| `Record`\<`string`, `number`\>

Modify the likelihood of specified tokens appearing in the completion.

Accepts a JSON object that maps tokens (specified by their token ID in the
tokenizer) to an associated bias value from -100 to 100. Mathematically, the
bias is added to the logits generated by the model prior to sampling. The exact
effect will vary per model, but values between -1 and 1 should decrease or
increase likelihood of selection; values like -100 or 100 should result in a ban
or exclusive selection of the relevant token.

#### Inherited from

`ChatCompletionCreateParamsBase.logit_bias`

***

### logprobs?

> `optional` **logprobs**: `null` \| `boolean`

Whether to return log probabilities of the output tokens or not. If true,
returns the log probabilities of each output token returned in the `content` of
`message`.

#### Inherited from

`ChatCompletionCreateParamsBase.logprobs`

***

### max\_completion\_tokens?

> `optional` **max\_completion\_tokens**: `null` \| `number`

An upper bound for the number of tokens that can be generated for a completion,
including visible output tokens and
[reasoning tokens](https://platform.openai.com/docs/guides/reasoning).

#### Inherited from

`ChatCompletionCreateParamsBase.max_completion_tokens`

***

### ~~max\_tokens?~~

> `optional` **max\_tokens**: `null` \| `number`

#### Deprecated

The maximum number of [tokens](/tokenizer) that can be generated in
the chat completion. This value can be used to control
[costs](https://openai.com/api/pricing/) for text generated via API.

This value is now deprecated in favor of `max_completion_tokens`, and is not
compatible with
[o-series models](https://platform.openai.com/docs/guides/reasoning).

#### Inherited from

`ChatCompletionCreateParamsBase.max_tokens`

***

### metadata?

> `optional` **metadata**: `null` \| `Metadata`

Set of 16 key-value pairs that can be attached to an object. This can be useful
for storing additional information about the object in a structured format, and
querying for objects via API or the dashboard.

Keys are strings with a maximum length of 64 characters. Values are strings with
a maximum length of 512 characters.

#### Inherited from

`ChatCompletionCreateParamsBase.metadata`

***

### modalities?

> `optional` **modalities**: `null` \| (`"audio"` \| `"text"`)[]

Output types that you would like the model to generate. Most models are capable
of generating text, which is the default:

`["text"]`

The `gpt-4o-audio-preview` model can also be used to
[generate audio](https://platform.openai.com/docs/guides/audio). To request that
this model generate both text and audio responses, you can use:

`["text", "audio"]`

#### Inherited from

`ChatCompletionCreateParamsBase.modalities`

***

### n?

> `optional` **n**: `null` \| `number`

How many chat completion choices to generate for each input message. Note that
you will be charged based on the number of generated tokens across all of the
choices. Keep `n` as `1` to minimize costs.

#### Inherited from

`ChatCompletionCreateParamsBase.n`

***

### parallel\_tool\_calls?

> `optional` **parallel\_tool\_calls**: `boolean`

Whether to enable
[parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
during tool use.

#### Inherited from

`ChatCompletionCreateParamsBase.parallel_tool_calls`

***

### prediction?

> `optional` **prediction**: `null` \| `ChatCompletionPredictionContent`

Static predicted output content, such as the content of a text file that is
being regenerated.

#### Inherited from

`ChatCompletionCreateParamsBase.prediction`

***

### presence\_penalty?

> `optional` **presence\_penalty**: `null` \| `number`

Number between -2.0 and 2.0. Positive values penalize new tokens based on
whether they appear in the text so far, increasing the model's likelihood to
talk about new topics.

#### Inherited from

`ChatCompletionCreateParamsBase.presence_penalty`

***

### reasoning\_effort?

> `optional` **reasoning\_effort**: `ReasoningEffort`

**o-series models only**

Constrains effort on reasoning for
[reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
result in faster responses and fewer tokens used on reasoning in a response.

#### Inherited from

`ChatCompletionCreateParamsBase.reasoning_effort`

***

### response\_format?

> `optional` **response\_format**: `ResponseFormatJSONObject` \| `ResponseFormatJSONSchema` \| `ResponseFormatText`

An object specifying the format that the model must output.

Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
Outputs which ensures the model will match your supplied JSON schema. Learn more
in the
[Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).

Setting to `{ "type": "json_object" }` enables the older JSON mode, which
ensures the message the model generates is valid JSON. Using `json_schema` is
preferred for models that support it.

#### Inherited from

`ChatCompletionCreateParamsBase.response_format`

***

### seed?

> `optional` **seed**: `null` \| `number`

This feature is in Beta. If specified, our system will make a best effort to
sample deterministically, such that repeated requests with the same `seed` and
parameters should return the same result. Determinism is not guaranteed, and you
should refer to the `system_fingerprint` response parameter to monitor changes
in the backend.

#### Inherited from

`ChatCompletionCreateParamsBase.seed`

***

### service\_tier?

> `optional` **service\_tier**: `null` \| `"flex"` \| `"auto"` \| `"default"`

Specifies the latency tier to use for processing the request. This parameter is
relevant for customers subscribed to the scale tier service:

- If set to 'auto', and the Project is Scale tier enabled, the system will
  utilize scale tier credits until they are exhausted.
- If set to 'auto', and the Project is not Scale tier enabled, the request will
  be processed using the default service tier with a lower uptime SLA and no
  latency guarantee.
- If set to 'default', the request will be processed using the default service
  tier with a lower uptime SLA and no latency guarantee.
- If set to 'flex', the request will be processed with the Flex Processing
  service tier.
  [Learn more](https://platform.openai.com/docs/guides/flex-processing).
- When not set, the default behavior is 'auto'.

When this parameter is set, the response body will include the `service_tier`
utilized.

#### Inherited from

`ChatCompletionCreateParamsBase.service_tier`

***

### stop?

> `optional` **stop**: `null` \| `string` \| `string`[]

Not supported with latest reasoning models `o3` and `o4-mini`.

Up to 4 sequences where the API will stop generating further tokens. The
returned text will not contain the stop sequence.

#### Inherited from

`ChatCompletionCreateParamsBase.stop`

***

### store?

> `optional` **store**: `null` \| `boolean`

Whether or not to store the output of this chat completion request for use in
our [model distillation](https://platform.openai.com/docs/guides/distillation)
or [evals](https://platform.openai.com/docs/guides/evals) products.

#### Inherited from

`ChatCompletionCreateParamsBase.store`

***

### stream\_options?

> `optional` **stream\_options**: `null` \| `ChatCompletionStreamOptions`

Options for streaming response. Only set this when you set `stream: true`.

#### Inherited from

`ChatCompletionCreateParamsBase.stream_options`

***

### temperature?

> `optional` **temperature**: `null` \| `number`

What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
make the output more random, while lower values like 0.2 will make it more
focused and deterministic. We generally recommend altering this or `top_p` but
not both.

#### Inherited from

`ChatCompletionCreateParamsBase.temperature`

***

### tool\_choice?

> `optional` **tool\_choice**: `ChatCompletionToolChoiceOption`

Controls which (if any) tool is called by the model. `none` means the model will
not call any tool and instead generates a message. `auto` means the model can
pick between generating a message or calling one or more tools. `required` means
the model must call one or more tools. Specifying a particular tool via
`{"type": "function", "function": {"name": "my_function"}}` forces the model to
call that tool.

`none` is the default when no tools are present. `auto` is the default if tools
are present.

#### Inherited from

`ChatCompletionCreateParamsBase.tool_choice`

***

### tools?

> `optional` **tools**: [`ChatCompletionTool`](ChatCompletionTool.md)[]

A list of tools the model may call. Currently, only functions are supported as a
tool. Use this to provide a list of functions the model may generate JSON inputs
for. A max of 128 functions are supported.

#### Inherited from

`ChatCompletionCreateParamsBase.tools`

***

### top\_logprobs?

> `optional` **top\_logprobs**: `null` \| `number`

An integer between 0 and 20 specifying the number of most likely tokens to
return at each token position, each with an associated log probability.
`logprobs` must be set to `true` if this parameter is used.

#### Inherited from

`ChatCompletionCreateParamsBase.top_logprobs`

***

### top\_p?

> `optional` **top\_p**: `null` \| `number`

An alternative to sampling with temperature, called nucleus sampling, where the
model considers the results of the tokens with top_p probability mass. So 0.1
means only the tokens comprising the top 10% probability mass are considered.

We generally recommend altering this or `temperature` but not both.

#### Inherited from

`ChatCompletionCreateParamsBase.top_p`

***

### user?

> `optional` **user**: `string`

A stable identifier for your end-users. Used to boost cache hit rates by better
bucketing similar requests and to help OpenAI detect and prevent abuse.
[Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).

#### Inherited from

`ChatCompletionCreateParamsBase.user`

***

### web\_search\_options?

> `optional` **web\_search\_options**: `WebSearchOptions`

This tool searches the web for relevant results to use in a response. Learn more
about the
[web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).

#### Inherited from

`ChatCompletionCreateParamsBase.web_search_options`

***

### stream?

> `optional` **stream**: `null` \| `false`

If set to true, the model response data will be streamed to the client as it is
generated using
[server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
See the
[Streaming section below](https://platform.openai.com/docs/api-reference/chat/streaming)
for more information, along with the
[streaming responses](https://platform.openai.com/docs/guides/streaming-responses)
guide for more information on how to handle the streaming events.

#### Overrides

`ChatCompletionCreateParamsBase.stream`
